# # -*- coding: utf-8 -*-
# """Half_Time_Draw_Prediction.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1Urcui2jPlXnDUaxy3_i8uyaOl-wVY4HI

# ## Data loading
# """


# import pandas as pd


# file_path = 'matches.json'
# try:
#     df = pd.read_json(file_path)
#     df.to_csv('/content/drive/MyDrive/ML Projects/Half_Time_Draw_Pred/matches.csv', index=False)  # Save to Drive
#     display(df.head())
# except FileNotFoundError:
#     print(f"Error: '{file_path}' not found.")
# except ValueError as e:
#     print(f"Error reading JSON: {e}")

# """## Data exploration

# """

# print("Shape of the DataFrame:", df.shape)
# print("\nData types of each column:\n", df.dtypes)
# print("\nDescriptive statistics:\n", df.describe())
# print("\nMissing values (NaNs) per column:\n", df.isnull().sum())
# print("\nPercentage of missing values per column:\n", (df.isnull().sum() / len(df)) * 100)
# print("\nUnique values in 'half_time_result' column:\n", df['halftime_result'].unique())

# """## Data cleaning


# """

# missing_percentage = df.isnull().sum() / len(df) * 100
# columns_to_drop = missing_percentage[missing_percentage > 50].index.tolist()
# df = df.drop(columns=columns_to_drop)
# numerical_cols = df.select_dtypes(include=['number']).columns
# for col in numerical_cols:
#     if df[col].isnull().any():
#         df[col] = df[col].fillna(df[col].median())
# categorical_cols = df.select_dtypes(exclude=['number']).columns
# for col in categorical_cols:
#     if df[col].isnull().any():
#         df[col] = df[col].fillna(df[col].mode()[0])

# df = df.drop_duplicates()

# display(df.head())
# print(df.isnull().sum())

# """## Data wrangling"""

# import pandas as pd

# df[['ht_home', 'ht_away']] = df['halftime_result'].str.extract(r'(\d+)\s*-\s*(\d+)').astype(int)
# df['half_time_draw'] = df.apply(lambda x: 'yes' if x['ht_home'] == x['ht_away'] else 'no', axis=1)
# df.drop(columns=['ht_home', 'ht_away'], inplace=True)
# columns_to_drop = ['halftime_result','date', 'hour', 'match', 'same_match', 'id','result']
# df = df.drop(columns=columns_to_drop, errors='ignore')

# display(df.head())

# df.columns

# print("\nData types of each column:\n", df.dtypes)

# """## Data analysis

# """

# print(df['half_time_draw'].value_counts())
# df_draws = df[df['half_time_draw'] == 'yes']
# print(df_draws[['home_goals', 'away_goals', 'home_goalsht', 'away_goalsht']].describe())
# print("Shape of df_draws:", df_draws.shape)

# """## Data preparation"""

# df['half_time_draw'] = df['half_time_draw'].map({'yes': 1, 'no': 0})
# display(df.head())

# """## Feature engineering"""

# # import pandas as pd
# # import matplotlib.pyplot as plt
# # import seaborn as sns

# # n = 20
# # correlation_matrix = df.corr()
# # target_correlations = correlation_matrix['half_time_draw']
# # top_correlated_features = target_correlations.drop('half_time_draw').nlargest(n, key=abs)  # Preserve sign
# # print(f"Top {n} highly correlated features with 'half_time_draw':")
# # print(top_correlated_features)

# # plt.figure(figsize=(12, 10))
# # sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
# # plt.title('Correlation Matrix')
# # plt.show()


# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns

# n = 32
# correlation_matrix = df.corr()
# target_correlations = correlation_matrix['half_time_draw'].abs()
# top_correlated_features = target_correlations.drop('half_time_draw').nlargest(n)
# print(f"Top {n} highly correlated features with 'half_time_draw':")
# print(top_correlated_features)
# plt.figure(figsize=(12, 10))
# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
# plt.title('Correlation Matrix')
# plt.show()

# """## Data splitting"""

# import pandas as pd
# import numpy as np
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import StandardScaler
# from sklearn.linear_model import LogisticRegression
# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
# import seaborn as sns
# import matplotlib.pyplot as plt
# y = df['half_time_draw']
# X = df.drop(columns=['half_time_draw'])
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
# numerical_features = X.select_dtypes(include=['number']).columns
# scaler = StandardScaler()
# X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])
# X_test[numerical_features] = scaler.transform(X_test[numerical_features])
# print("\nDataset Splits:")
# print("X_train shape:", X_train.shape)
# print("y_train shape:", y_train.shape)
# print("X_test shape:", X_test.shape)
# print("y_test shape:", y_test.shape)

"""## Model training"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC
import joblib
import os
import warnings
warnings.filterwarnings('ignore')

def split_data(df, target_col='half_time_draw', test_size=0.2, random_state=42):
    """
    Split the data into training and test sets
    
    Parameters:
    -----------
    df : pd.DataFrame
        DataFrame to split
    target_col : str
        Name of the target column
    test_size : float
        Proportion of data to use for testing
    random_state : int
        Random seed for reproducibility
        
    Returns:
    --------
    tuple
        (X_train, X_test, y_train, y_test) - Split data
    """
    print("\n===== SPLITTING DATA =====")
    
    # Separate features and target
    X = df.drop(columns=[target_col])
    y = df[target_col]
    
    # Split data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )
    
    print(f"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(df)*100:.1f}%)")
    print(f"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(df)*100:.1f}%)")
    
    # Check class distribution in each set
    print("\nClass distribution:")
    print(f"Training set: {y_train.value_counts(normalize=True).mul(100).round(1).to_dict()}")
    print(f"Test set: {y_test.value_counts(normalize=True).mul(100).round(1).to_dict()}")
    
    return X_train, X_test, y_train, y_test

def train_logistic_regression(X_train, y_train, class_weight=None):
    """
    Train a logistic regression model
    
    Parameters:
    -----------
    X_train : array-like
        Training features
    y_train : array-like
        Training target
    class_weight : dict or 'balanced', optional
        Class weights for imbalanced data
        
    Returns:
    --------
    LogisticRegression
        Trained logistic regression model
    """
    print("\n===== TRAINING LOGISTIC REGRESSION =====")
    
    # Define parameter grid - optimized for efficiency
    param_grid = {
        'C': [0.1, 1, 10],
        'penalty': ['l2'],  # Reduced to only l2 for faster training
        'solver': ['liblinear'],
        'class_weight': [None, 'balanced']
    }
    
    # Create and train the model with grid search
    grid_search = GridSearchCV(
        LogisticRegression(max_iter=1000, random_state=42),
        param_grid=param_grid,
        cv=3,  # Reduced from 5 to 3 for faster training
        scoring='f1',
        n_jobs=-1
    )
    
    grid_search.fit(X_train, y_train)
    
    # Get the best model
    best_model = grid_search.best_estimator_
    
    # Print best parameters
    print(f"Best parameters: {grid_search.best_params_}")
    
    # Print cross-validation results
    print(f"Cross-validation F1 score: {grid_search.best_score_:.4f}")
    
    return best_model

def train_random_forest(X_train, y_train, class_weight=None):
    """
    Train a random forest model
    
    Parameters:
    -----------
    X_train : array-like
        Training features
    y_train : array-like
        Training target
    class_weight : dict or 'balanced', optional
        Class weights for imbalanced data
        
    Returns:
    --------
    RandomForestClassifier
        Trained random forest model
    """
    print("\n===== TRAINING RANDOM FOREST =====")
    
    # Define parameter grid - reduced to speed up training
    param_grid = {
        'n_estimators': [100, 200],
        'max_depth': [None, 10],
        'min_samples_split': [2, 5],
        'min_samples_leaf': [1, 2],
        'class_weight': [None, 'balanced']
    }
    
    # Create and train the model with grid search
    grid_search = GridSearchCV(
        RandomForestClassifier(random_state=42),
        param_grid=param_grid,
        cv=3,  # Reduced from 5 to 3 to speed up training
        scoring='f1',
        n_jobs=-1
    )
    
    grid_search.fit(X_train, y_train)
    
    # Get the best model
    best_model = grid_search.best_estimator_
    
    # Print best parameters
    print(f"Best parameters: {grid_search.best_params_}")
    
    # Print cross-validation results
    print(f"Cross-validation F1 score: {grid_search.best_score_:.4f}")
    
    # Print feature importances
    if hasattr(X_train, 'columns'):
        feature_importances = pd.DataFrame({
            'feature': X_train.columns,
            'importance': best_model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print("\nTop 10 feature importances:")
        print(feature_importances.head(10))
    
    return best_model

def train_gradient_boosting(X_train, y_train):
    """
    Train a gradient boosting model
    
    Parameters:
    -----------
    X_train : array-like
        Training features
    y_train : array-like
        Training target
        
    Returns:
    --------
    GradientBoostingClassifier
        Trained gradient boosting model
    """
    print("\n===== TRAINING GRADIENT BOOSTING =====")
    
    # Define parameter grid - optimized for efficiency
    param_grid = {
        'n_estimators': [100, 200],
        'learning_rate': [0.01, 0.1],
        'max_depth': [3, 5],
        'min_samples_split': [2, 5],
        'subsample': [0.8]
    }
    
    # Create and train the model with grid search
    grid_search = GridSearchCV(
        GradientBoostingClassifier(random_state=42),
        param_grid=param_grid,
        cv=3,  # Reduced from 5 to 3 for faster training
        scoring='f1',
        n_jobs=-1
    )
    
    grid_search.fit(X_train, y_train)
    
    # Get the best model
    best_model = grid_search.best_estimator_
    
    # Print best parameters
    print(f"Best parameters: {grid_search.best_params_}")
    
    # Print cross-validation results
    print(f"Cross-validation F1 score: {grid_search.best_score_:.4f}")
    
    # Print feature importances
    if hasattr(X_train, 'columns'):
        feature_importances = pd.DataFrame({
            'feature': X_train.columns,
            'importance': best_model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print("\nTop 10 feature importances:")
        print(feature_importances.head(10))
    
    return best_model

def train_xgboost(X_train, y_train):
    """
    Train an XGBoost model
    
    Parameters:
    -----------
    X_train : array-like
        Training features
    y_train : array-like
        Training target
        
    Returns:
    --------
    XGBClassifier
        Trained XGBoost model
    """
    print("\n===== TRAINING XGBOOST =====")
    
    # Define parameter grid - optimized for efficiency
    param_grid = {
        'n_estimators': [100, 200],
        'learning_rate': [0.01, 0.1],
        'max_depth': [3, 5],
        'min_child_weight': [1, 5],
        'subsample': [0.8],
        'colsample_bytree': [0.8]
    }
    
    # Create and train the model with grid search
    grid_search = GridSearchCV(
        XGBClassifier(random_state=42, eval_metric='logloss'),
        param_grid=param_grid,
        cv=3,  # Reduced from 5 to 3 for faster training
        scoring='f1',
        n_jobs=-1
    )
    
    grid_search.fit(X_train, y_train)
    
    # Get the best model
    best_model = grid_search.best_estimator_
    
    # Print best parameters
    print(f"Best parameters: {grid_search.best_params_}")
    
    # Print cross-validation results
    print(f"Cross-validation F1 score: {grid_search.best_score_:.4f}")
    
    # Print feature importances
    if hasattr(X_train, 'columns'):
        feature_importances = pd.DataFrame({
            'feature': X_train.columns,
            'importance': best_model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print("\nTop 10 feature importances:")
        print(feature_importances.head(10))
    
    return best_model

def evaluate_model(model, X_test, y_test, model_name, save_dir=None):
    """
    Evaluate a model on the test set
    
    Parameters:
    -----------
    model : object
        Trained model
    X_test : array-like
        Test features
    y_test : array-like
        Test target
    model_name : str
        Name of the model
    save_dir : str or None
        Directory to save visualizations, if None, visualizations are not saved
        
    Returns:
    --------
    dict
        Dictionary with evaluation metrics
    """
    print(f"\n===== EVALUATING {model_name.upper()} =====")
    
    # Import save_visualization function from util
    from util import save_visualization
    
    # Make predictions
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    
    # Print metrics
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    
    # Print classification report
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))
    
    # Plot confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    fig_cm = plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['No Draw', 'Draw'], 
                yticklabels=['No Draw', 'Draw'])
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix - {model_name}')
    plt.tight_layout()
    if save_dir:
        save_visualization(fig_cm, f'confusion_matrix_{model_name.lower().replace(" ", "_")}.png', save_dir)
    #plt.show()
    
    # Plot ROC curve
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    roc_auc = auc(fpr, tpr)
    
    fig_roc = plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {model_name}')
    plt.legend(loc="lower right")
    plt.tight_layout()
    if save_dir:
        save_visualization(fig_roc, f'roc_curve_{model_name.lower().replace(" ", "_")}.png', save_dir)
    #plt.show()
    
    # Plot precision-recall curve
    precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba)
    pr_auc = auc(recall_curve, precision_curve)
    
    fig_pr = plt.figure(figsize=(8, 6))
    plt.plot(recall_curve, precision_curve, color='blue', lw=2, label=f'PR curve (area = {pr_auc:.2f})')
    plt.axhline(y=sum(y_test)/len(y_test), color='red', linestyle='--', label=f'Baseline (ratio = {sum(y_test)/len(y_test):.2f})')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(f'Precision-Recall Curve - {model_name}')
    plt.legend(loc="lower left")
    plt.tight_layout()
    if save_dir:
        save_visualization(fig_pr, f'pr_curve_{model_name.lower().replace(" ", "_")}.png', save_dir)
    #plt.show()
    
    # Return metrics
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'roc_auc': roc_auc,
        'pr_auc': pr_auc
    }

def compare_models(models, X_test, y_test, save_dir=None):
    """
    Compare multiple models
    
    Parameters:
    -----------
    models : dict
        Dictionary with model names as keys and trained models as values
    X_test : array-like
        Test features
    y_test : array-like
        Test target
    save_dir : str or None
        Directory to save visualizations, if None, visualizations are not saved
        
    Returns:
    --------
    str
        Name of the best model based on F1 score
    """
    print("\n===== MODEL COMPARISON =====")
    
    # Import save_visualization function from util
    from util import save_visualization
    
    # Collect metrics for each model
    metrics = {}
    for name, model in models.items():
        metrics[name] = evaluate_model(model, X_test, y_test, name, save_dir)
    
    # Create a DataFrame for comparison
    comparison_df = pd.DataFrame(metrics).T
    comparison_df = comparison_df.round(4)
    
    # Print comparison
    print("\nModel Comparison:")
    print(comparison_df)
    
    # Save comparison to CSV
    if save_dir:
        os.makedirs(save_dir, exist_ok=True)
        comparison_df.to_csv(os.path.join(save_dir, 'model_comparison.csv'))
        print(f"Model comparison saved to {os.path.join(save_dir, 'model_comparison.csv')}")
    
    # Plot comparison
    fig = plt.figure(figsize=(12, 8))
    comparison_df.plot(kind='bar', figsize=(12, 8))
    plt.title('Model Comparison')
    plt.ylabel('Score')
    plt.xlabel('Model')
    plt.xticks(rotation=0)
    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=3)
    plt.tight_layout()
    if save_dir:
        save_visualization(fig, 'model_comparison.png', save_dir)
    #plt.show()
    
    # Find the best model based on F1 score
    best_model_name = comparison_df['f1'].idxmax()
    print(f"\nBest model based on F1 score: {best_model_name}")
    
    return best_model_name

def save_model(model, model_name, output_dir='models'):
    """
    Save a trained model
    
    Parameters:
    -----------
    model : object
        Trained model
    model_name : str
        Name of the model
    output_dir : str
        Directory to save the model
        
    Returns:
    --------
    str
        Path to the saved model
    """
    # Create directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Save model
    model_path = os.path.join(output_dir, f"{model_name}.joblib")
    joblib.dump(model, model_path)
    print(f"Model saved to {model_path}")
    
    return model_path

def load_model(model_path):
    """
    Load a trained model
    
    Parameters:
    -----------
    model_path : str
        Path to the saved model
        
    Returns:
    --------
    object
        Loaded model
    """
    model = joblib.load(model_path)
    print(f"Model loaded from {model_path}")
    
    return model